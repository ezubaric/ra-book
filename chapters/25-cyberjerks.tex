
% Macros for character names
% Should probably move to the actual fiction chapter
\newcommand{\salesjerk}[0]{Levi}

\chapter{AI can be Jerks: Learning from the Best}

The previous story saw our protagonist start her carrer, and our
essays will begin with foundational material.  What a lay user needs
to understand what's happening in the story and how realistic it is.
We'll save the more fanciful and complicated stuff for later chapters.

For the moment, we focus on a facet of artificial intelligence called
machine learning.  Machine learning is a small subset of artificial
intelligence,\footnote{Research is organized around conferences; these
  onferences often form a community that is protective of its turf.
  While there is substantial overlap between the communities (e.g.,
  the Neural Information Processing Systems conference), many machine
  learning researchers (e.g., the International Conference on Machine
  Learning) prefer to remain distinct from ``pure'' artificial
  intelligence.} but unlike much of the fancifal claims of aritificial
intelligence prophets, it actually works \emph{now}.

We first introduce a key component of machine learning---objective
functions---that define why systems act they way they do. We then show
how bad human behavior can be mimicked by these algorithms (it's
already happening!).  Finally, we close the chapter with a call to
action: keep your computers safe!

\section{Learning by Doing: Objective Functions}

Let us begin with an essential and underappreciated example of machine
learning that simply works and makes modern life liveable: spam
filters.  An e-mail is either spam or not; computers think in ones and
zeros, so let's call spam a one and not spam a zero.

An algorithm does not just say ``yes'' or ``no'' to whether an e-mail
is spam.  It typically gives a score for each e-mail: the higher the
score, the more confident it is the e-mail is spam.  For the moment,
let's assume that all of the numbers are between zero and one (e.g.,
they are a probability).

A perfect score would be if all spam e-mails got a 1.0 and all of the
good e-mails got 0.0.  Both perfection and absolute certainty are
unattainable goals, so we need to deal with both errors and
uncertainty.  So we sum all of the spam documents and see how far the
scores are from 1.0, and we sum all of the good e-mails and see how
far the scores are from 0.0.  This sum is our \emph{objective
  function}: a perfect score is 0.0 and the worst score is the number
of e-mails.

Machine learning algorithms are defined by \emph{parameters}.  For
example, a parameter might define what the algorithm does when it sees
the word ``viagra'' or the phrase ``I am a Nigerian prince''.  These
parameters effectively define the algorithm; machine learning
algorithms learn how to set these parameters from data, a process that
can take multiple names,\footnote{Some of the alternate names are
  field specific; for example, in statistics this is called inference
  and the objective function is typically called the likelihood.
  However, the mechanics are similar.  For some applications, it's
  called a ``loss function'', but we'll stick with the more general
  term.} but we'll call it ``optimization''.

\subsection{Optimization}

Optimization is the process where machine learning uses data to find
the parameters that give the best score on the objective function.
Typically, the process starts with a random setting of the parameters.
This will do \emph{horribly}, but it provides a place to start: the
early improvements will be easy.

The learning process will slowly change the parameters to ever so
slightly improve the objective function.\footnote{Mathematically, this
  is usually by looking at the derivative (if you remember that from your calculus class) of the objective function with
  respect to the parameters.}  The learning process goes document by
document to slowly improve the objective function.  Eventually, modern
optimization techniques find a setting of the parameters that work
well for this problem.\footnote{The form of the objective function
  determines whether this answer will be the best possible or merely a
  ``good'' solution.}

Let's take a concrete example.  Let's say that we have an e-mail that
includes the phrase ``special offer'' and the algorithm says that it
is spam with score 0.7.  This is a spam e-mail, so the algorithm could
do better in terms of its objective function if the parameter
associated with the phrase ``special offer'' were more strongly
associated with spam.  So our optimization will push the parameter a
little more in that direction so that the score is 0.75 the next time
it sees a similar document.  This continues until the parameters
cannot get any better.

All of this seems fairly straightforward, but there is some art to
creating these systems.  Defining the parameters requires a bit of
expertise: either hand-crafting parameters that fit the problem well
or using automatic approaches that learn effective representations.
Doing this well requires quite a bit of work, and often your first
(and second, etc.) attempt will usually fail.

\subsection{Is this Artificial Intelligence?}

You might be skeptical about whether this is actually artificial
intelligence.  We have a function and we are optimizing the
parameters.  Whether this is actually \emph{intelligence} is
debateable.

A skeptic would point out that this is a mechanical mathematical
exercise.  The algorithm that decides whether an e-mail is spam or not
doesn't actually understand what it's reading, and the parameters were
defined by a human.  While the values of the parameters come from
data, there's no real learning going on.

A proponent would counter that intelligence is actually a combination
of many smaller processes.  To understand an e-mail, we need to
understand parts of speech, syntax, discourse, and pragmatics.  Each
of these can be thought of as individual problems like spam
classification.  Each one is mechanistic, but together they form a
process that can be viewed as intelligent.

For the moment, we will leave the debate there.
Chapter~\ref{chap:gai} takes up the question about what it means to be
intelligent and how to measure the intelligence of algorithms.

Nevertheless, machine learning has earned the reputation of being
``artificial intelligence that works''.  Its mechanistic definition is
a virtue: it is easy to see what works and what does not.  Just as the
objective function allows the algorithm to tune individual parameters
to improve a single task, the clear defintion allows researchers to
tweak problems, models, and data to get better at important tasks.

Machine learning has dones well at many tasks\dots

\subsection{Training Data}

This machine learning setup,\footnote{Formally, this setup is called
  \emph{supervised machine learning}, where there are specific
  input--output examples the algorithm must replicate.  Unsupervised
  machine learning is another active area of research, but it is much
  harder to see when things are working well.  We talk about
  applications and evaluations of unsupervised machine learning in
  Section~\ref{sec:unsup}} however, only works with generous training
data.  Modern machine learning algorithms are also notoriously data
hungry.  They can always do better, but they can only do better at the
cost of additional training data: if you see input~$x$, what should
your output~$y$ be?

These training data have to come from somewhere.  

\section{Humans are Jerks}

Redlining

\subsection{I Learned it from You!}

\subsection{Prevention is Tough}

\section{Practicing Good Technology Hygene}

Stuxnet
